{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter-3-generating-handwritten-digits-using-gans.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMyaweAhKHJ+VDXTXs6GJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/gans-in-action/blob/part-1-introduction-to-gans-and-generative-modeling/chapter_3_generating_handwritten_digits_using_gans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRa3V41yXCB",
        "colab_type": "text"
      },
      "source": [
        "# Generating handwritten digits using GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgGCA-ERy4cQ",
        "colab_type": "text"
      },
      "source": [
        "We will implement a GAN that learns to produce realistic-looking handwritten digits. We will use the Python neural network library Keras with a TensorFlow backend.\n",
        "\n",
        "Over the course of the training iterations, the Generator learns to turn random noise input into images that look like members of the training data: the MNIST dataset of handwritten digits. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/gan-network.png?raw=1' width='800'/>\n",
        "\n",
        "Simultaneously, the Discriminator learns to distinguish the fake images produced by the Generator from the genuine ones coming from the training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMFaRsxq0Dcq",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ripW-zhr0Erm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.datasets import  mnist\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKAMQKJF0_P",
        "colab_type": "text"
      },
      "source": [
        "We specify the input dimensions of our model and dataset. Each image in MNIST is 28 × 28 pixels with a single channel (because the images are grayscale). The variable z_dim sets the size of the noise vector, z."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4bjBP1D0lgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "# Input image dimensions\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "# Size of the noise vector, used as input to the Generator\n",
        "z_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kODT4P_oGJsm",
        "colab_type": "text"
      },
      "source": [
        "##  Implementing the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XTc-38GNZC",
        "colab_type": "text"
      },
      "source": [
        "For simplicity, the Generator is a neural network with only a single hidden layer. It takes in z as input and produces a 28 × 28 × 1 image. In the hidden layer, we use the Leaky ReLU activation function. \n",
        "\n",
        "Unlike a regular ReLU function, which maps any negative input to 0, Leaky ReLU allows a small positive gradient. This prevents gradients from dying out during training, which tends to yield better training outcomes.\n",
        "\n",
        "At the output layer, we employ the tanh activation function, which scales the output values to the range $[–1, 1]$. The reason for using tanh (as opposed to, say, sigmoid, which would output values in the more typical 0 to 1 range) is that tanh tends to produce crisper images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQxZmD8hGHMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(img_shape, z_dim):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=z_dim))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Dense(28 * 28 * 1, activation='tanh'))\n",
        "  # Reshape the Generator output to image dimensions\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srEYzgpMH4cX",
        "colab_type": "text"
      },
      "source": [
        "## Implementing the Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neUaX4a0H5tK",
        "colab_type": "text"
      },
      "source": [
        "The Discriminator takes in a 28 × 28 × 1 image and outputs a probability indicating whether the input is deemed real ratherthan fake. The Discriminator is represented by a two-layer neural network, with 128 hidden units and a Leaky ReLU activation function at the hidden layer.\n",
        "\n",
        "For simplicity, our Discriminator network looks almost identical to the Generator. This does not have to be the case; indeed, in most GAN implementations, the Generator and Discriminator network architectures vary greatly in both size and complexity.\n",
        "\n",
        "Notice that unlike for the Generator, in the following listing we apply the sigmoid activation function at the Discriminator’s output layer. This ensures that our output value will be between 0 and 1, so it can be interpreted as the probability the Generator assigns that the input is real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0oe05jiHyVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "\n",
        "  model.add(Dense(128))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOoOHZbwJkiF",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIg89es2JliY",
        "colab_type": "text"
      },
      "source": [
        "Notice that in the combined model used to train the Generator, we keep the Discriminator parameters fixed by setting discriminator.trainable to False. Also note that the combined model, in which the Discriminator is set to untrainable, is used to train the Generator only. The Discriminator is trained as an independently compiled model. \n",
        "\n",
        "We use binary cross-entropy as the loss function we are seeking to minimize during training. Binary cross-entropy is a measure of the difference between computed probabilities and actual probabilities for predictions with only two possible classes. The greater the cross-entropy loss, the further away our predictions are from the true labels.\n",
        "\n",
        "To optimize each network, we use the Adam optimization algorithm. This algorithm, whose name is derived from adaptive moment estimation, is an advanced gradient-descent-based optimizer. Adam has become the go-to optimizer for most GAN implementations thanks to its often superior performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt5eunwEJgHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "  model = Sequential()\n",
        "\n",
        "  # Combined Generator -> Discriminator model\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Build and compile the Discriminator\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Build the Generator\n",
        "generator = build_generator(img_shape, z_dim)\n",
        "\n",
        "# Keep Discriminator’s parameters constant for Generator training\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Build and compile GAN model with fixed Discriminator to train the Generator\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm8I1RB2Mu2j",
        "colab_type": "text"
      },
      "source": [
        "## Training GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXuz4XWMzbS",
        "colab_type": "text"
      },
      "source": [
        "We get a random mini-batch of MNIST images as real examples and generate a mini-batch of fake images from random noise vectors z. We then use those to train the Discriminator network while keeping the Generator’s parameters constant. \n",
        "\n",
        "Next we generate a mini-batch of fake images and use those to train the Generator network while keeping the Discriminator’s parameters fixed.\n",
        "We repeat this for each iteration.\n",
        "\n",
        "We use one-hot-encoded labels: 1 for real images and 0 for fake ones. To generate z, we sample from the standard normal distribution (a bell curve with 0 mean and a standard deviation of 1). \n",
        "\n",
        "The Discriminator is trained to assign fake labels to the fake images and real labels to real images. The Generator is trained such that the Discriminator assigns real labels to the fake examples it produces.\n",
        "\n",
        "Notice that we are rescaling the real images in the training dataset from –1 to 1. As you saw in the preceding example, the Generator uses the tanh activation function at the output layer, so the fake images will be in the range (–1, 1). Accordingly, we have to rescale all the Discriminator’s inputs to the same range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRB4XOL6MyfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}