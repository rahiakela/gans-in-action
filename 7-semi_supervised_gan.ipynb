{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi-supervised-gan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoVrE5HkUjfB9iu68gyOEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/gans-in-action/blob/part-2-advanced-topics-in-gans/7-semi_supervised_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pizKqGIkp91",
        "colab_type": "text"
      },
      "source": [
        "# Semi-Supervised GAN(SGAN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjA_5Gotkq1o",
        "colab_type": "text"
      },
      "source": [
        "Semi-supervised learning is one of the most promising areas of practical application of GANs. \n",
        "\n",
        "Unlike supervised learning, in which we need a label for every example in our\n",
        "dataset, and unsupervised learning, in which no labels are used, semi-supervised\n",
        "learning has a class label for only a small subset of the training dataset. \n",
        "\n",
        "By internalizing hidden structures in the data, semi-supervised learning strives to generalize from the small subset of labeled data points to effectively classify new, previously unseen examples. \n",
        "\n",
        "Importantly, for semi-supervised learning to work, the labeled and unlabeled\n",
        "data must come from the same underlying distribution.\n",
        "\n",
        "Interestingly, semi-supervised learning may also be one of the closest machine\n",
        "learning analogs to the way humans learn. \n",
        "\n",
        "When schoolchildren learn to read and write, the teacher does not have to take them on a road trip to see tens of thousands of examples of letters and numbers, ask them to identify these symbols, and correct them as needed—similarly to the way a supervised learning algorithm would operate.\n",
        "Instead, a single set of examples is all that is needed for children to learn letters and numerals and then be able to recognize them regardless of font, size, angle, lighting conditions, and many other factors. Semi-supervised learning aims to teach machines in a similarly efficient manner.\n",
        "\n",
        "Serving as a source of additional information that can be used for training, generative models proved useful in improving the accuracy of semi-supervised models. Unsurprisingly, GANs have proven the most promising."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-jY5AClg05",
        "colab_type": "text"
      },
      "source": [
        "## What is a Semi-Supervised GAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l9AyIbLliCG",
        "colab_type": "text"
      },
      "source": [
        "Semi-Supervised GAN (SGAN) is a Generative Adversarial Network whose Discriminator is a multiclass classifier. Instead of distinguishing between only two classes (real and fake), it learns to distinguish between N + 1 classes, where N is the number of classes in the training dataset, with one added for the fake examples produced by the Generator.\n",
        "\n",
        "For example, the MNIST dataset of handwritten digits has 10 labels (one label for each numeral, 0 to 9), so the SGAN Discriminator trained on this dataset would predict between 10 + 1 = 11 classes. \n",
        "\n",
        "In our implementation, the output of the SGAN Discriminator will be represented as a vector of 10 class probabilities (that sum up to 1.0) plus another probability that represents whether the image is real or fake.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/sgan-architecture.png?raw=1' width='800'/>\n",
        "\n",
        "---\n",
        "\n",
        "In this Semi-Supervised GAN, the Generator takes in a random noise\n",
        "vector z and produces a fake example x*. The Discriminator receives three kinds of data inputs: fake data from the Generator, real unlabeled examples x, and real labeled examples (x, y), where y is the label corresponding to the given example. The Discriminator then outputs a classification; its goal is to distinguish fake examples from the real ones and, for the real examples, identify the correct class. Notice that the portion of examples with labels is much smaller than the portion of the unlabeled data. In practice, the contrast is even starker than the one shown, with labeled data forming only a tiny fraction (often as little as 1–2%) of the training data.\n",
        "\n",
        "---\n",
        "\n",
        "The task of distinguishing between multiple classes not only\n",
        "impacts the Discriminator itself, but also adds complexity to the SGAN architecture, its training process, and its training objectives, as compared to the traditional GAN.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFAyB1rHoWAp",
        "colab_type": "text"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VV2AsYoY21",
        "colab_type": "text"
      },
      "source": [
        "The SGAN Generator’s purpose is the same as in the original GAN: it takes in a vector of random numbers and produces fake examples whose goal is to be indistinguishable from the training dataset—no change here.\n",
        "\n",
        "The SGAN Discriminator, however, diverges considerably from the original GAN\n",
        "implementation. Instead of two, it receives three kinds of inputs: fake examples produced by the Generator (x*), real examples without labels from the training dataset (x), and real examples with labels from the training dataset (x, y), where y denotes the label for the given example x.Instead of binary classification, the SGAN Discriminator’s goal is to correctly categorize the input example into its corresponding class if the example is real, or reject the example as fake (which can be thought of as a special additional class).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/sgan-network-table.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FaSgDE4pCzf",
        "colab_type": "text"
      },
      "source": [
        "## Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bryN-FoppE-B",
        "colab_type": "text"
      },
      "source": [
        "In a regular GAN, we train the Discriminator by computing the loss for\n",
        "$D(x)$ and $D(x*)$ and backpropagating the total loss to update the Discriminator’s trainable parameters to minimize the loss. The Generator is trained by backpropagating the Discriminator’s loss for D(x*), seeking to maximize it, so that the fake examples it synthesizes are misclassified as real.\n",
        "\n",
        "To train the SGAN, in addition to D(x) and D(x*), we also have to compute the loss for the supervised training examples: D((x, y)). These losses correspond to the dual learning objective that the SGAN Discriminator has to grapple with: distinguishing real examples from the fake ones while also learning to classify real examples to their correct classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PvJ20SVpd5F",
        "colab_type": "text"
      },
      "source": [
        "## Training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLzCaKO_pezF",
        "colab_type": "text"
      },
      "source": [
        "All the GAN generative models goal is to produce realistic-looking data samples; hence, the Generator network has been of primary interest. The main purpose of the Discriminator network has been to help the Generator improve the quality of images it produces. \n",
        "\n",
        "**At the end of the training, we often disregard the Discriminator and use only the fully trained Generator to create realistic-looking synthetic data.**\n",
        "\n",
        "In contrast, in a SGAN, we care primarily about the Discriminator. The goal of the training process is to make this network into a semi-supervised classifier whose accuracy is as close as possible to a fully supervised classifier (one that has labels available for each example in the training dataset), while using only a small fraction of the labels. The Generator’s goal is to aid this process by serving as a source of additional information (the fake data it produces) that helps the Generator learn the relevant patterns in the data, enhancing its classification accuracy. \n",
        "\n",
        "**At the end of the training, the Generator gets discarded, and we use the trained Discriminator as a classifier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xs6iB1YqoP7",
        "colab_type": "text"
      },
      "source": [
        "## Implementing a Semi-Supervised GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-B3TzELqqae",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}