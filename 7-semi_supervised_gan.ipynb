{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semi-supervised-gan.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkwbw52l1kDSLuZnF/lQtu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/gans-in-action/blob/part-2-advanced-topics-in-gans/7-semi_supervised_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pizKqGIkp91",
        "colab_type": "text"
      },
      "source": [
        "# Semi-Supervised GAN(SGAN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjA_5Gotkq1o",
        "colab_type": "text"
      },
      "source": [
        "Semi-supervised learning is one of the most promising areas of practical application of GANs. \n",
        "\n",
        "Unlike supervised learning, in which we need a label for every example in our\n",
        "dataset, and unsupervised learning, in which no labels are used, semi-supervised\n",
        "learning has a class label for only a small subset of the training dataset. \n",
        "\n",
        "By internalizing hidden structures in the data, semi-supervised learning strives to generalize from the small subset of labeled data points to effectively classify new, previously unseen examples. \n",
        "\n",
        "Importantly, for semi-supervised learning to work, the labeled and unlabeled\n",
        "data must come from the same underlying distribution.\n",
        "\n",
        "Interestingly, semi-supervised learning may also be one of the closest machine\n",
        "learning analogs to the way humans learn. \n",
        "\n",
        "When schoolchildren learn to read and write, the teacher does not have to take them on a road trip to see tens of thousands of examples of letters and numbers, ask them to identify these symbols, and correct them as needed—similarly to the way a supervised learning algorithm would operate.\n",
        "Instead, a single set of examples is all that is needed for children to learn letters and numerals and then be able to recognize them regardless of font, size, angle, lighting conditions, and many other factors. Semi-supervised learning aims to teach machines in a similarly efficient manner.\n",
        "\n",
        "Serving as a source of additional information that can be used for training, generative models proved useful in improving the accuracy of semi-supervised models. Unsurprisingly, GANs have proven the most promising."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-jY5AClg05",
        "colab_type": "text"
      },
      "source": [
        "## What is a Semi-Supervised GAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l9AyIbLliCG",
        "colab_type": "text"
      },
      "source": [
        "Semi-Supervised GAN (SGAN) is a Generative Adversarial Network whose Discriminator is a multiclass classifier. Instead of distinguishing between only two classes (real and fake), it learns to distinguish between N + 1 classes, where N is the number of classes in the training dataset, with one added for the fake examples produced by the Generator.\n",
        "\n",
        "For example, the MNIST dataset of handwritten digits has 10 labels (one label for each numeral, 0 to 9), so the SGAN Discriminator trained on this dataset would predict between 10 + 1 = 11 classes. \n",
        "\n",
        "In our implementation, the output of the SGAN Discriminator will be represented as a vector of 10 class probabilities (that sum up to 1.0) plus another probability that represents whether the image is real or fake.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/sgan-architecture.png?raw=1' width='800'/>\n",
        "\n",
        "---\n",
        "\n",
        "In this Semi-Supervised GAN, the Generator takes in a random noise\n",
        "vector z and produces a fake example x*. The Discriminator receives three kinds of data inputs: fake data from the Generator, real unlabeled examples x, and real labeled examples (x, y), where y is the label corresponding to the given example. The Discriminator then outputs a classification; its goal is to distinguish fake examples from the real ones and, for the real examples, identify the correct class. Notice that the portion of examples with labels is much smaller than the portion of the unlabeled data. In practice, the contrast is even starker than the one shown, with labeled data forming only a tiny fraction (often as little as 1–2%) of the training data.\n",
        "\n",
        "---\n",
        "\n",
        "The task of distinguishing between multiple classes not only\n",
        "impacts the Discriminator itself, but also adds complexity to the SGAN architecture, its training process, and its training objectives, as compared to the traditional GAN.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFAyB1rHoWAp",
        "colab_type": "text"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VV2AsYoY21",
        "colab_type": "text"
      },
      "source": [
        "The SGAN Generator’s purpose is the same as in the original GAN: it takes in a vector of random numbers and produces fake examples whose goal is to be indistinguishable from the training dataset—no change here.\n",
        "\n",
        "The SGAN Discriminator, however, diverges considerably from the original GAN\n",
        "implementation. Instead of two, it receives three kinds of inputs: fake examples produced by the Generator (x*), real examples without labels from the training dataset (x), and real examples with labels from the training dataset (x, y), where y denotes the label for the given example x.Instead of binary classification, the SGAN Discriminator’s goal is to correctly categorize the input example into its corresponding class if the example is real, or reject the example as fake (which can be thought of as a special additional class).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/sgan-network-table.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FaSgDE4pCzf",
        "colab_type": "text"
      },
      "source": [
        "## Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bryN-FoppE-B",
        "colab_type": "text"
      },
      "source": [
        "In a regular GAN, we train the Discriminator by computing the loss for\n",
        "$D(x)$ and $D(x*)$ and backpropagating the total loss to update the Discriminator’s trainable parameters to minimize the loss. The Generator is trained by backpropagating the Discriminator’s loss for D(x*), seeking to maximize it, so that the fake examples it synthesizes are misclassified as real.\n",
        "\n",
        "To train the SGAN, in addition to D(x) and D(x*), we also have to compute the loss for the supervised training examples: D((x, y)). These losses correspond to the dual learning objective that the SGAN Discriminator has to grapple with: distinguishing real examples from the fake ones while also learning to classify real examples to their correct classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PvJ20SVpd5F",
        "colab_type": "text"
      },
      "source": [
        "## Training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLzCaKO_pezF",
        "colab_type": "text"
      },
      "source": [
        "All the GAN generative models goal is to produce realistic-looking data samples; hence, the Generator network has been of primary interest. The main purpose of the Discriminator network has been to help the Generator improve the quality of images it produces. \n",
        "\n",
        "**At the end of the training, we often disregard the Discriminator and use only the fully trained Generator to create realistic-looking synthetic data.**\n",
        "\n",
        "In contrast, in a SGAN, we care primarily about the Discriminator. The goal of the training process is to make this network into a semi-supervised classifier whose accuracy is as close as possible to a fully supervised classifier (one that has labels available for each example in the training dataset), while using only a small fraction of the labels. The Generator’s goal is to aid this process by serving as a source of additional information (the fake data it produces) that helps the Generator learn the relevant patterns in the data, enhancing its classification accuracy. \n",
        "\n",
        "**At the end of the training, the Generator gets discarded, and we use the trained Discriminator as a classifier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xs6iB1YqoP7",
        "colab_type": "text"
      },
      "source": [
        "## Implementing a Semi-Supervised GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-B3TzELqqae",
        "colab_type": "text"
      },
      "source": [
        "We implement an SGAN model that learns to classify handwritten digits in the MNIST dataset by using only 100 training examples. At the end of it, we compare the model’s classification accuracy to an equivalent fully supervised model to see for ourselves the improvement achieved by semi-supervised learning.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gans-in-action/sgan-diagram.png?raw=1' width='800'/>\n",
        "\n",
        "The Generator turns random noise into fake examples. The Discriminator receives real images with labels (x, y), real images without labels (x), and fake images produced by the Generator (x*). To distinguish real examples from fake ones, the Discriminator uses the sigmoid function. To distinguish between the real classes, the Discriminator uses the\n",
        "softmax function.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "To solve the multiclass classification problem of distinguishing between the real labels, the Discriminator uses the softmax function, which gives probability distribution over a specified number of classes—in our case, 10. The higher the probability assigned to a given label, the more confident the Discriminator is that the example\n",
        "belongs to the given class. To compute the classification error, we use cross-entropy loss, which measures the difference between the output probabilities and the target,\n",
        "one-hot-encoded labels.\n",
        "\n",
        "To output the real-versus-fake probability, the Discriminator uses the sigmoid activation function and trains its parameters by backpropagating the binary cross-entropy loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHR5V7Franbx",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iwZ9Roxa3lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import (Activation, BatchNormalization, Concatenate, Dense, Dropout,\n",
        "                                     Flatten, Input, Lambda, Reshape, LeakyReLU, Conv2D, Conv2DTranspose)\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSe2XTk4dEnf",
        "colab_type": "text"
      },
      "source": [
        "We also specify the input image size, the size of the noise vector $z$, and the number of the real classes for the semi-supervised classification (one for each numeral our Discriminator will learn to identify), as shown in the following listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FZAuMHMcT9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model input dimensions\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "# Input image dimensions\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "# Size of the noise vector, used as input to the Generator\n",
        "z_dim = 100\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkrTK1oUeaBl",
        "colab_type": "text"
      },
      "source": [
        "### The dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKuIFYa3ehMR",
        "colab_type": "text"
      },
      "source": [
        "Although the MNIST training dataset has `50,000` labeled training images, we will use only a small fraction of them (specified by the `num_labeled` parameter) for training\n",
        "and pretend that all the remaining ones are unlabeled. We accomplish this by sampling only from the first num_labeled images when generating batches of labeled data and from the remaining `(50,000 – num_labeled)` images when generating batches of unlabeled examples.\n",
        "\n",
        "The Dataset object also provides a function to return all the\n",
        "`num_labeled` training examples along with their labels as well as a function to return all `10,000` labeled test images in the MNIST dataset. After training, we will use the test set to evaluate how well the model’s classifications generalize to previously unseen examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVLjVXPFfTMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "\n",
        "  def __init__(self, num_labeled):\n",
        "    # Number of labeled examples to use for training\n",
        "    self.num_labeled = num_labeled\n",
        "    # Load the MNIST dataset\n",
        "    (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
        "\n",
        "    def preprocess_imgs(x):\n",
        "      # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
        "      x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "      # Expand image dimensions to width x height x channels\n",
        "      x = np.expand_dims(x, axis=3)\n",
        "      return x\n",
        "\n",
        "    def preprocess_labels(y):\n",
        "      return y.reshape(1, -1)\n",
        "\n",
        "    # Training data\n",
        "    self.x_train = preprocess_imgs(self.x_train)\n",
        "    self.y_train = preprocess_labels(self.y_train)\n",
        "\n",
        "    # Testing data\n",
        "    self.x_test = preprocess_imgs(self.x_test)\n",
        "    self.y_test = preprocess_labels(self.y_test)\n",
        "  \n",
        "  def batch_labeled(self, batch_size):\n",
        "    # Get a random batch of labeled images and their labels\n",
        "    idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "    imgs = self.x_train[idx]\n",
        "    labels = self.y_train[idx]\n",
        "    return imgs, labels\n",
        "\n",
        "  def batch_unlabeled(self, batch_size):\n",
        "    # Get a random batch of unlabeled images\n",
        "    idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "    imgs = self.x_train[idx]\n",
        "    return imgs\n",
        "\n",
        "  def training_set(self):\n",
        "    x_train = self.x_train[range(self.num_labeled)]\n",
        "    y_train = self.y_train[range(self.num_labeled)]\n",
        "    return x_train, y_train\n",
        "\n",
        "  def test_set(self):\n",
        "    return self.x_test, self.y_test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOdcFD_0aVeI",
        "colab_type": "text"
      },
      "source": [
        "We will pretend that we have only 100 labeled MNIST images for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZmpFuifaQpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c7060ce-7eda-4c82-c4d0-8121bc8abdfb"
      },
      "source": [
        "# Number of labeled examples to use (rest will be used as unlabeled)\n",
        "num_labeled = 100\n",
        "\n",
        "dataset = Dataset(num_labeled)\n",
        "print(dataset.num_labeled)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re1CdkG7bJI1",
        "colab_type": "text"
      },
      "source": [
        "### The Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T62BoNWb8AT",
        "colab_type": "text"
      },
      "source": [
        "The Generator network is the same as the one we implemented for the DCGAN. Using transposed convolution layers, the Generator transforms the input random noise vector into 28 × 28 × 1 image; see the following listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrNfzvqxan09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}